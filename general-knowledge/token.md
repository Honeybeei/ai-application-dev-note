# 🪙 Token

- [🪙 Token](#-token)
  - [❓ What is Token \& Tokenization?](#-what-is-token--tokenization)
  - [📚 Tokenization Techniques](#-tokenization-techniques)
  - [🤔 Why Tokenization is important?](#-why-tokenization-is-important)
  - [🪟 Context Window](#-context-window)

## ❓ What is Token & Tokenization?

> **Token** is a smallest unit of text that can be processed by a language model. It can be a word, a subword, or a character. **Tokenization** is the process of splitting a text into tokens.

## 📚 Tokenization Techniques

There are several techniques for tokenization but here, we will discuss the most common ones:

- **Character Tokenization**: Splitting a text into characters.
- **Word Tokenization**: Splitting a text into words.
- **Subword Tokenization**: Splitting a text into subwords. It splits words into more flexible units that can be recombined to represent different words.

## 🤔 Why Tokenization is important?

> Tokenization is important because it is the first step in processing text data. It helps to break down the text into smaller units that can be easily processed by the language model. By choosing the right tokenization technique, we can improve the performance of the language model which means better results with less computational resources.

## 🪟 Context Window

> In a language model, the **context window** refers to the amount of text